<!doctype html>
<html lang="en" dir="ltr" class="blog-wrapper blog-tags-post-list-page plugin-blog plugin-id-default">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v2.2.0">
<title data-rh="true">2 posts tagged with &quot;scalability&quot; | CNOE</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:url" content="https://cnoe-io.github.io/blog/tags/scalability"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" property="og:title" content="2 posts tagged with &quot;scalability&quot; | CNOE"><meta data-rh="true" name="docusaurus_tag" content="blog_tags_posts"><meta data-rh="true" name="docsearch:docusaurus_tag" content="blog_tags_posts"><link data-rh="true" rel="icon" href="/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://cnoe-io.github.io/blog/tags/scalability"><link data-rh="true" rel="alternate" href="https://cnoe-io.github.io/blog/tags/scalability" hreflang="en"><link data-rh="true" rel="alternate" href="https://cnoe-io.github.io/blog/tags/scalability" hreflang="x-default"><link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="CNOE RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="CNOE Atom Feed">



<link rel="stylesheet" href="https://fonts.googleapis.com/icon?family=Material+Icons"><link rel="stylesheet" href="/assets/css/styles.4061a733.css">
<link rel="preload" href="/assets/js/runtime~main.b0d8bc33.js" as="script">
<link rel="preload" href="/assets/js/main.e84582e8.js" as="script">
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}()</script><div id="__docusaurus">
<div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#docusaurus_skipToContent_fallback">Skip to main content</a></div><nav class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/logo.png" alt="CNOE Logo" class="themedImage_ToTc themedImage--light_HNdA"><img src="/img/logo.png" alt="CNOE Logo" class="themedImage_ToTc themedImage--dark_i4oU"></div><b class="navbar__title text--truncate"></b></a><a class="navbar__item navbar__link" href="/docs/category/deploy-a-platform">Deploy</a><a class="navbar__item navbar__link" href="/docs/intro">Concepts</a><a class="navbar__item navbar__link" href="/radars">Technology Radars</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/blog">Blog</a></div><div class="navbar__items navbar__items--right"><a href="https://calendar.google.com/calendar/u/0/embed?src=064a2adfce866ccb02e61663a09f99147f22f06374e7a8994066bdc81e066986@group.calendar.google.com&amp;ctz=America/Los_Angeles" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link navbar--calendar-link" aria-label="Community Calendar"></a><a href="https://cloud-native.slack.com/archives/C05TN9WFN5S" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link navbar--slack-link" aria-label="Slack Channel"></a><a href="https://github.com/cnoe-io" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link navbar--github-link" aria-label="GitHub Repository"></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="searchBox_ZlJk"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0"><div class="container margin-vert--lg"><div class="row"><aside class="col col--3"><nav class="sidebar_re4s thin-scrollbar" aria-label="Blog recent posts navigation"><div class="sidebarItemTitle_pO2u margin-bottom--md">Recent posts</div><ul class="sidebarItemList_Yudw clean-list"><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/argo-workflow-scalability">Argo Workflows Controller Scalability Testing on Amazon EKS</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/argo-cd-application-scalability">Argo CD Benchmarking - Pushing the Limits and Sharding Deep Dive</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/optimizing-data-quality-in-dev-portals">Optimizing for Data Quality in your Developer Portal</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/welcome">CNOE - A Joint Effort to Share Internal Developer Platform Tools and Best Practices.</a></li></ul></nav></aside><main class="col col--7" itemscope="" itemtype="http://schema.org/Blog"><header class="margin-bottom--xl"><h1>2 posts tagged with &quot;scalability&quot;</h1><a href="/blog/tags">View All Tags</a></header><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><meta itemprop="image" content="https://cnoe.io/assets/images/image30-3953ec55f6ce3f531ad8325ac8a7e0f1.png"><header><h2 class="title_f1Hy" itemprop="headline"><a itemprop="url" href="/blog/argo-workflow-scalability">Argo Workflows Controller Scalability Testing on Amazon EKS</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2024-06-04T00:00:00.000Z" itemprop="datePublished">June 4, 2024</time> · <!-- -->18 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://github.com/Enclavet" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo" src="https://avatars.githubusercontent.com/u/2309880" alt="Andrew Lee"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://github.com/Enclavet" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Andrew Lee</span></a></div><small class="avatar__subtitle" itemprop="description">Architect, AWS</small></div></div></div><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://github.com/vsethi" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo" src="https://avatars.githubusercontent.com/u/10545494?v=4" alt="Vikram Sethi"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://github.com/vsethi" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Vikram Sethi</span></a></div><small class="avatar__subtitle" itemprop="description">Computer Scientist, Adobe</small></div></div></div></div></header><div class="markdown" itemprop="articleBody"><h2 class="anchor anchorWithStickyNavbar_LWe7" id="introduction"><strong>Introduction</strong><a class="hash-link" href="#introduction" title="Direct link to heading">​</a></h2><p>In our earlier blog posts, we have discussed scalability tests for Argo CD, where in two consecutive experiments, we pushed the limits of Argo CD to deploy <a href="https://aws.amazon.com/blogs/opensource/argo-cd-application-controller-scalability-testing-on-amazon-eks/" target="_blank" rel="noopener noreferrer">10,000 applications on ~100 clusters</a> and then<a href="https://cnoe.io/blog/argo-cd-application-scalability" target="_blank" rel="noopener noreferrer"> 50,000 applications on 500 clusters</a> along with configuration and fine-tuning required to make Argo CD scale effectively. Argo CD deployments, however, do not happen in isolation, and similar to a <a href="https://cnoe.io/docs/reference-implementation" target="_blank" rel="noopener noreferrer">CNOE stack</a>, Argo CD is often deployed on a cluster along with other tooling which collectively contribute to the performance and scalability bottlenecks we see users run into. </p><p>Argo Workflows is one common tool we often see users deploy alongside Argo CD to enable workflow executions (e.g. building images, running tests, cutting releases, etc). Our early experiments with Argo Workflows revealed that, if not tuned properly, it can negatively impact the scalability of a given Kubernetes cluster, particularly if the Kubernetes cluster happens to be the control cluster managing developer workflows across a large group of users. A real world example of some of the scaling challenges you can encounter with Argo Workflows is explored in our recent ArgoCon talk: <a href="https://www.youtube.com/watch?v=7yVXMCX62tY" target="_blank" rel="noopener noreferrer">Key Takeaways from Scaling Adobe&#x27;s CI/CD Solution to Support 50K Argo CD Apps</a>.</p><p>For us to better understand the limitations and tuning requirements for Argo Workflows, in this blog post we publish details on the scalability experiments we ran for Argo Workflows executing Workflows in two different load patterns across 50 Amazon EKS nodes. We show the correlation between the various Argo Workflow’s knobs and controls and the processing time as well as performance improvements you can get by determining how you supply the workflows to the control plane.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="test-parameters"><strong>Test Parameters</strong><a class="hash-link" href="#test-parameters" title="Direct link to heading">​</a></h2><h3 class="anchor anchorWithStickyNavbar_LWe7" id="test-workflow"><strong>Test Workflow</strong><a class="hash-link" href="#test-workflow" title="Direct link to heading">​</a></h3><p>The test workflow is based on the lightweight whalesay container from docker which prints out some text and ASCII art to the terminal. The reason we chose a lightweight container is that we wanted to stress the Argo Workflows controller in managing the Workflow lifecycle (pod creation, scheduling, and cleanup) and minimize the extra overhead on the Kubernetes control plane in dealing with the data plane workloads. An example of the Workflow is below:</p><div class="language-go codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-go codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token keyword" style="color:#00009f">var</span><span class="token plain"> helloWorldWorkflow </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> wfv1</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">Workflow</span><span class="token punctuation" style="color:#393A34">{</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    ObjectMeta</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> metav1</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">ObjectMeta</span><span class="token punctuation" style="color:#393A34">{</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        GenerateName</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;hello-world-&quot;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token punctuation" style="color:#393A34">}</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    Spec</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> wfv1</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">WorkflowSpec</span><span class="token punctuation" style="color:#393A34">{</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        Entrypoint</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;whalesay&quot;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        ServiceAccountName</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;argo&quot;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        Templates</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">[</span><span class="token punctuation" style="color:#393A34">]</span><span class="token plain">wfv1</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">Template</span><span class="token punctuation" style="color:#393A34">{</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            </span><span class="token punctuation" style="color:#393A34">{</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                Name</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;whalesay&quot;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                Container</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> </span><span class="token operator" style="color:#393A34">&amp;</span><span class="token plain">corev1</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">Container</span><span class="token punctuation" style="color:#393A34">{</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                    Image</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain">   </span><span class="token string" style="color:#e3116c">&quot;docker/whalesay:latest&quot;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                    Command</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">[</span><span class="token punctuation" style="color:#393A34">]</span><span class="token builtin">string</span><span class="token punctuation" style="color:#393A34">{</span><span class="token string" style="color:#e3116c">&quot;cowsay&quot;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;hello world&quot;</span><span class="token punctuation" style="color:#393A34">}</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                </span><span class="token punctuation" style="color:#393A34">}</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            </span><span class="token punctuation" style="color:#393A34">}</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token punctuation" style="color:#393A34">}</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        PodGC</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> </span><span class="token operator" style="color:#393A34">&amp;</span><span class="token plain">wfv1</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">PodGC</span><span class="token punctuation" style="color:#393A34">{</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            Strategy</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;OnPodSuccess&quot;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token punctuation" style="color:#393A34">}</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token punctuation" style="color:#393A34">}</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token punctuation" style="color:#393A34">}</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><h3 class="anchor anchorWithStickyNavbar_LWe7" id="argo-workflows-settings"><strong>Argo Workflows Settings</strong><a class="hash-link" href="#argo-workflows-settings" title="Direct link to heading">​</a></h3><p>We will be detailing how each of these settings affect Argo Workflow in various experiments later in this blog post.</p><ul><li><p>Controller workers: Argo Workflows controller utilizes different workers for various operations in a Workflow lifecycle. We will be looking at t types of workers for our scalability testing.</p><ul><li><p>workflow-workers (default: 32): These workers are threads in a single Argo Workflows controller that reconcile Argo Workflow Custom Resources (CRs). When a Workflow is created, a workflow-worker will handle the end-to-end operations of the Workflow from ensuring the pod is scheduled to ensuring the pod has finished. The number of workers can be specified by passing the <code>--workflow-workers</code> flag to the controller.</p></li><li><p>pod-cleanup-workers (default: 4): These workers clean up finished Workflows. When a Workflow has finished executing, depending on your clean-up settings, a pod-cleanup-worker will handle cleaning up the pod from the Workflow. The number of workers can be specified by passing the <code>--pod-cleanup-workers</code> flag to the controller.</p></li></ul></li><li><p>Client queries per second (QPS)/Burst QPS settings (default: 20/30): These settings control when the Argo Workflows controller’s Kubernetes (K8s) client starts to throttle requests to the K8S API server. The client QPS setting is for limiting sustained QPS for the k8s client while burst QPS is for allowing a burst request rate in excess of the client QPS for a short period of time. The client QPS/burst QPS can be set by passing the <code>--qps</code> and <code>--burst</code> flag to the controller.</p></li><li><p>Sharding: Sharding with multiple Argo Workflows controllers is possible by running each controller in its own namespace. The controller would only reconcile Workflows submitted in that particular namespace. The namespace of each controller can be specified with the <code>--namespaced</code> flag.</p></li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="key-metrics"><strong>Key Metrics</strong><a class="hash-link" href="#key-metrics" title="Direct link to heading">​</a></h2><p>We chose a set of key metrics for the scalability testing because we wanted to measure how many workflows the Argo Workflows controller can reconcile and process. We will also be looking into K8s control plane metrics which might indicate your control plane cannot keep up with the Argo Workflows workload. </p><ul><li><p>Workqueue depth: The workqueue depth shows workflows which have not been reconciled. If the depth starts to increase, it indicates that the Argo Workflows controller is unable to handle the submission rate of Workflows.</p></li><li><p>Workqueue latency: The workqueue latency is the average time workflows spent waiting in the workqueue. A lower value indicates that the Argo Workflows controller is processing workflows faster so that they are not waiting in the workqueue.</p></li><li><p>K8S api server requests per second: The read and write requests per second being made to the K8S api server.</p></li></ul><p>We didn’t include CPU/Memory as a key metric because during our testing we did not see any significant impacts to both. Most likely because of our simplistic workflows utilized for this benchmark.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="environment">Environment<a class="hash-link" href="#environment" title="Direct link to heading">​</a></h2><p>We ran the experiments in an AWS environment utilizing a single Amazon EKS cluster. The Kubernetes version is 1.27 and Argo Workflows version is 3.5.4. No resource quotas were utilized on the Argo Workflows controller. For the cluster, we will start by provisioning 1x m5.8xlarge Amazon Elastic Compute Cloud (Amazon EC2) instances which will run the Argo Workflows controller and 50x m5.large instances for executing workflows. The number of execution instances is sufficient to run all 5000 workflows in parallel to ensure that pods are not waiting on resources to execute. Monitoring and metrics for Argo Workflows were provided by Prometheus/Grafana. </p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="methodology"><strong>Methodology</strong><a class="hash-link" href="#methodology" title="Direct link to heading">​</a></h2><p>There will be two types of load patterns evaluated:</p><p><strong>Increasing Rate Test:</strong> Workflows will be submitted at an increasing rate (workflows/min) until the Argo Workflows controller cannot keep up. The state at which the controller cannot keep up is when there are &gt;0 workflows in the workflow queue or there is increasing queue latency. That rate of Workflow submissions will be noted as the maximum rate at which the Argo Workflows can be processed with the current settings. </p><p><strong>Queued Reconciliation Test: </strong>5000 workflows are submitted in less than minute. Metrics will be monitored from when the Argo Workflows controller starts processing workflows to when it has reconciled all 5000 workflows. The number of nodes is sufficient for running all the workflows simultaneously.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="experiments">Experiments<a class="hash-link" href="#experiments" title="Direct link to heading">​</a></h2><h3 class="anchor anchorWithStickyNavbar_LWe7" id="experiment-1-baseline">Experiment 1: Baseline<a class="hash-link" href="#experiment-1-baseline" title="Direct link to heading">​</a></h3><p>In our baseline experiment, we are running in a single Argo Workflows shard (namespace) with default settings.</p><p><strong>Increasing Rate Test:</strong></p><p>As you can see below, the Argo Workflows controller can process up to 270 workflows/min. The average workqueue latency and workqueue depth are nearly zero.  At 300 workflows/min, workqueue latency and workqueue depth starts to increase.</p><p><img loading="lazy" alt="Enter image alt description" src="/assets/images/1-d6f2f88df7f56060c4c8a765a82dcbfb.png" width="974" height="753" class="img_ev3q"></p><p><strong>Queued Reconciliation Test:</strong></p><p>It takes around 17 mins to reconcile 5000 workflows and peak avg workqueue latency was 5.38 minutes.</p><p><img loading="lazy" alt="Enter image alt description" src="/assets/images/2-211e5d9c30190ab7a797043dcdc689e4.png" width="972" height="459" class="img_ev3q"></p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="experiment-2-workflow-workers">Experiment 2: Workflow Workers<a class="hash-link" href="#experiment-2-workflow-workers" title="Direct link to heading">​</a></h3><p>For this experiment, we increase the number of workflow workers from the default of 32 to 128 where the workers use the maximum QPS and burst settings available to them. We also had to increase the number of pod-cleanup-workers to 32 as the Argo Workflows controller was experiencing some instability, where the controller pod was consistently crashing with the default value of 4. </p><p><strong>Increasing Rate Test:</strong></p><p>For the increasing workflow rate test, we can see exactly when the number of workflow workers is not sufficient to process the load. Both workqueue latency and depth start to increase indicating that workflows are waiting to be reconciled. When we increase the number of workers, the controller is able to reconcile the current load until an additional load is placed on it. For 32 workers, that limit is 300 workflows/min. When we increase the number of workers to 64, it is able to process that load until load is increased to 330 workflows/min. Then we increase the number of workers to 96 and it can process the additional load again. When we increase to 360 workflows/min, we need to bump the number of workers to 128.</p><table><thead><tr><th>Workers</th><th>Max workflows/minute</th></tr></thead><tbody><tr><td>32</td><td>270</td></tr><tr><td>64</td><td>300</td></tr><tr><td>96</td><td>330</td></tr><tr><td>128</td><td>360</td></tr></tbody></table><p><img loading="lazy" alt="Enter image alt description" src="/assets/images/3-116cb38fa45fd79d00154959c7823fe9.png" width="971" height="760" class="img_ev3q"></p><p>For the K8S api server, we see sustained 180 writes/sec and 70 reads/sec during the increasing rate tests.</p><p><img loading="lazy" alt="Enter image alt description" src="/assets/images/AYf_Image_1-a76b61333fac9118e74c3538c6bb6ef6.png" width="937" height="787" class="img_ev3q"></p><p><strong>Queued Reconciliation Test:</strong></p><p>For the queued reconciliation test, the time it took to reconcile all the workflows did not change significantly. With 32 workers it took 17 mins to reconcile while with 96 workers it took 16 mins. The peak workqueue latency did decrease from 5.38 mins with 32 workers to 3.19 mins with 96 workers. With 128 workers, the Argo Workflows controller kept crashing.</p><table><thead><tr><th>Workers</th><th>Peak avg latency (mins)</th><th>Reconcile time (mins)</th></tr></thead><tbody><tr><td>32</td><td>5.38</td><td>17</td></tr><tr><td>64</td><td>5.06</td><td>18</td></tr><tr><td>96</td><td>3.19</td><td>16</td></tr><tr><td>128</td><td>N/A</td><td>N/A</td></tr></tbody></table><p><img loading="lazy" alt="Enter image alt description" src="/assets/images/4-c8a05383e3dd19b4f35f4ddf168e9fbb.png" width="971" height="487" class="img_ev3q"></p><p>For the K8S api server, we see peaks of up to 260 writes/sec and 90 reads/sec during the queued reconciliation tests. You notice for the last test that there is no K8S api server activity as the Argo Workflows controller was misbehaving due to client-side throttling.</p><p><img loading="lazy" alt="Enter image alt description" src="/assets/images/ZlY_Image_2-5b7ab12762820f727e0a8b792c505da2.png" width="942" height="795" class="img_ev3q"></p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="observations-from-experiment-2"><strong>Observations from Experiment 2:</strong><a class="hash-link" href="#observations-from-experiment-2" title="Direct link to heading">​</a></h4><p>Workers play a big part in how fast the Argo Workflows controller is able to reconcile the rate of workflows being submitted. If you are observing workflow latency and backing up the workqueue depth, changing the number of workers is a potential way to improve performance. There are a few observations that we want to call out. One is that if we compare the two different patterns, one where we submit workflows at a constant rate and one in which we load up the workqueue all at once, we can see variations in calculated throughput. We can actually calculate the time it takes to reconcile 5000 apps utilizing the increasing rate test results and compare them to the queued reconciliation test.</p><table><thead><tr><th>Workers</th><th>Increasing rate test time to reconciling 5000 workflows (mins)</th><th>Reconcile time of 5000 workflows queued all at once (mins)</th></tr></thead><tbody><tr><td>32</td><td>18.5</td><td>17</td></tr><tr><td>64</td><td>16.6</td><td>18</td></tr><tr><td>96</td><td>15.1</td><td>16</td></tr><tr><td>128</td><td>13.8</td><td>N/A</td></tr></tbody></table><p>We do get some conflicting results when we make this comparison. With 32 and 64 workers, the increasing rate test is actually slower than the queued reconciliation test. But if we increase to 96 workers, we can see that the increasing rate test results are faster. We were unable to compare with 128 workers as the Argo Workflows controller crashed when trying to run the queued reconciliation test. When investigating the cause of the crash, the logs have several messages like the following:</p><div class="language-log codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-log codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">Waited for 6.185558715s due to client-side throttling, not priority and fairness, request: DELETE:https://10.100.0.1:443/api/v1/namespaces/argoworkflows1/pods/hello-world-57cfda8a-dc8b-4854-83a0-05785fb25e4b-3gwthk</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>These messages indicate that we should increase the Client QPS settings which we will evaluate in the next experiment.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="experiment-3-client-qps-settings">Experiment 3: Client QPS Settings<a class="hash-link" href="#experiment-3-client-qps-settings" title="Direct link to heading">​</a></h3><p>For this experiment, we set the number of workflow workers back to the default of 32. We will then increase the QPS/Burst by increments of 10/10, from 20/30 to 50/60. We chose to only increase by 10/10 because any large increase past 50/60 did not yield any performance improvements. We believe that this is partly because we kept the workers at 32.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="initial-testing"><strong>Initial Testing</strong><a class="hash-link" href="#initial-testing" title="Direct link to heading">​</a></h4><p><strong>Increasing Rate Test:</strong></p><p>The QPS/Burst settings had a significant impact on the increasing rate test. By increasing the QPS/Burst from 20/30 to 30/40, we see ~50% improvement in max workflows/min from 270 to 420. When we increase the QPS/Burst from 30/40 to 40/50, we see another 28% improvement in max workflows/min from 420 to 540. When increasing from 40/50 to 50/60 there was only an additional 5% improvement. For 32 workers, increasing past 50/60 did not yield any significant improvements to the max workflows/min.</p><table><thead><tr><th>QPS/Burst</th><th>Max workflows/minute</th></tr></thead><tbody><tr><td>20/30</td><td>270</td></tr><tr><td>30/40</td><td>420</td></tr><tr><td>40/50</td><td>540</td></tr><tr><td>50/60</td><td>570</td></tr></tbody></table><p><img loading="lazy" alt="Enter image alt description" src="/assets/images/ZgU_Image_3-9ca2e433fea080701af4ce97b19c38b2.png" width="1379" height="1049" class="img_ev3q"></p><p>When changing QPS/Burst, we need to also monitor the K8S API server. Looking at the K8S API server req/s, we see sustained 390 writes/sec and 85 read/sec.</p><p><img loading="lazy" alt="Enter image alt description" src="/assets/images/DNV_Image_4-182669a9a3fd061ba0b23f7f7c0cf017.png" width="939" height="786" class="img_ev3q"></p><p><strong>Queued Reconciliation Test:</strong></p><p>Again, the QPS/Burst settings make a big difference in the queued reconciliation test when compared to just changing the workflow workers. Starting from the default settings of 20/30, we see decreasing reconcile times from 19 mins to 12 mins to 8 mins and finally to 6 mins when setting the QPS/Burst to 50/60. The peak average latency also decreased from 4.79 mins to 1.94 mins. We did note that there was a higher peak avg latency with 30/40 vs 20/30 but if you examine the graph you can see a steeper drop in latency accounting for the shorter reconcile time. Similar to the increasing rate test, increasing the QPS/Burst further did not yield any improvements.</p><table><thead><tr><th>QPS/Burst</th><th>Peak avg latency (mins)</th><th>Reconcile time (mins)</th></tr></thead><tbody><tr><td>20/30</td><td>4.79</td><td>19</td></tr><tr><td>30/40</td><td>5.66</td><td>12</td></tr><tr><td>40/50</td><td>2.98</td><td>8</td></tr><tr><td>50/60</td><td>1.94</td><td>6</td></tr></tbody></table><p><img loading="lazy" alt="Enter image alt description" src="/assets/images/lGk_Image_5-7155dee2d602ff28c6ba4957a700c26a.png" width="1376" height="656" class="img_ev3q"></p><p>When looking at the K8S API server, we see peaks of up to 700 writes/sec and 200 reads/sec during the tests.</p><p><img loading="lazy" alt="Enter image alt description" src="/assets/images/Bpv_Image_6-c62a64840de7c331ae4a926dbe3232c8.png" width="947" height="791" class="img_ev3q"></p><p>When compared to the workflow workers testing, you can see increasing the QPS/Burst is able to push the K8S API server and improve Argo Workflows overall performance. We do see some diminishing returns when increasing QPS/Burst past 50/60 even though it appears that the K8S API server has plenty of capacity for additional load. For the next test, we will increase both the workflow workers with the QPS/burst to see how far we can push Argo Workflows and the K8s API server.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="max-load-test"><strong>Max Load Test</strong><a class="hash-link" href="#max-load-test" title="Direct link to heading">​</a></h4><p><strong>Increasing Rate Test:</strong></p><p>We increased the number of workers to 128 and QPS/burst to 60/70 and observed peak average latency of 54 secs and a reconciliation time of 5 mins. Increasing either the workers or QPS/Burst did not improve these numbers.</p><p><img loading="lazy" alt="Enter image alt description" src="/assets/images/czp_Image_7-c58e659986c95fa3f0542fc70c6c0bbd.png" width="1520" height="708" class="img_ev3q"></p><p>Looking at the K8s API server, we saw peaks of 800 writes/sec and 190 reads/sec.</p><p><img loading="lazy" alt="Enter image alt description" src="/assets/images/o0Z_Image_8-de223d0852717da7581e45b5b4a3ee6b.png" width="944" height="793" class="img_ev3q"></p><p><strong>Queued Reconciliation Test:</strong></p><p>Starting with 128 workers and QPS/Burst of 60/70, we were able to push Argo Workflows to 810 workflows/min. But past that point, there were no improvements with more workers or increased QPS/Burst limits.</p><p><img loading="lazy" alt="Enter image alt description" src="/assets/images/S1V_Image_9-8517f8a643c7bfaf787f916c9478746e.png" width="1532" height="1052" class="img_ev3q"></p><p>We can see increased K8s API server activity with sustained 700 writes/sec and 160 reads/sec.</p><p><img loading="lazy" alt="Enter image alt description" src="/assets/images/NRO_Image_10-84aacbe750ca5f604e939f2f628bcd8e.png" width="945" height="790" class="img_ev3q"></p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="observations-from-experiment-3"><strong>Observations from Experiment 3</strong><a class="hash-link" href="#observations-from-experiment-3" title="Direct link to heading">​</a></h4><p>One observation we made in the previous experiment with workflow workers is that the two different patterns of submitting workflows can be compared. We made that comparison again with the QPS/Burst tests and saw the following results:</p><table><thead><tr><th>QPS/Burst</th><th>Workers</th><th>Increasing rate test time to reconcile 5000 workflows (mins)</th><th>Reconcile time of 5000 workflows queued all at once (mins)</th></tr></thead><tbody><tr><td>20/30</td><td>32</td><td>18.5</td><td>19</td></tr><tr><td>30/40</td><td>32</td><td>11.9</td><td>12</td></tr><tr><td>50/60</td><td>32</td><td>9.2</td><td>8</td></tr><tr><td>60/70</td><td>32</td><td>8.7</td><td>6</td></tr><tr><td>70/80</td><td>128</td><td>6.1</td><td>5</td></tr></tbody></table><p>When we take the data about the comparison in experiment 1 with the data above, we can see a slight improvement in submitting all workflows together vs staggering them. We are not sure why this is the case and more experiments are required to understand this behavior.</p><p>It seems that we have hit a wall with 128 workers and a QPS/burst of 60/70 for a single Argo Workflows Controller. We will now evaluate Sharding and see if we can improve our performance from this point.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="experiment-4-sharding">Experiment 4: Sharding<a class="hash-link" href="#experiment-4-sharding" title="Direct link to heading">​</a></h3><p>For this experiment, we will evaluate 1 shard, 2 shards, and 5 shards of the Argo Workflows controller with the default settings. We will then try for a maximum load test utilizing workflow workers, QPS/burst, and sharding to see the maximum performance on our current infrastructure.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="initial-testing-1"><strong>Initial Testing</strong><a class="hash-link" href="#initial-testing-1" title="Direct link to heading">​</a></h4><p><strong>Increasing Rate Test:</strong></p><p>Sharding the Argo Workflows controller has a linear impact on performance with the increasing rate test. By increasing the number of shards from 1 to 2, we see a 100% improvement in max workflows/min from 270 to 540. When we increase the shards from 2 to 5, we see an additional 150% improvement in max workflows/min from 540 to 1350.</p><table><thead><tr><th>Shards</th><th>Max workflows/min</th></tr></thead><tbody><tr><td>1</td><td>270</td></tr><tr><td>2</td><td>540</td></tr><tr><td>5</td><td>1350</td></tr></tbody></table><p>One thing to note is that each shard is increased by 30 workflows/min when increasing the rate. This means that the difference between two rates with 2 shards <em> 30 = 60 workflows/min and the difference between two rates with 5 shards </em> 30 = 150 workflows/min. That is why for 2 shards when the max load was determined at 600 workflows/min, we go down 1 rate which is 600 - 60 = 540 workflows/min.</p><p><img loading="lazy" alt="Enter image alt description" src="/assets/images/WP9_Image_11-d8f7b5848f2adde883ab39b81f4d2cd0.png" width="1532" height="1038" class="img_ev3q"></p><p>You can see a significant impact on the K8s API server with sustained 1400 writes/sec and 300 reads/sec.</p><p><img loading="lazy" alt="Enter image alt description" src="/assets/images/3gj_Image_12-506b3b1f517fda7c68995a3a6eda6c33.png" width="940" height="789" class="img_ev3q"></p><p><strong>Queued Reconciliation Test:</strong></p><p>As shown in the Increasing Rate Test, sharding has a huge impact on performance for the queued reconciliation test. With 1 shard it takes 18 mins to reconcile 5000 workflows, while with 2 shards it takes 9 mins. With 5 shards the reconcile time is further reduced to 4 mins.</p><table><thead><tr><th>Shards</th><th>Peak avg latency (mins)</th><th>Reconcile time (mins)</th></tr></thead><tbody><tr><td>1</td><td>5.43</td><td>18</td></tr><tr><td>2</td><td>3.81</td><td>9</td></tr><tr><td>5</td><td>1.42</td><td>4</td></tr></tbody></table><p><img loading="lazy" alt="Enter image alt description" src="/assets/images/6Bi_Image_13-9f8b7d60ba060017942a0bd9fd4e6d57.png" width="1535" height="699" class="img_ev3q"></p><p>The impact on the K8s API server was not as significant when compared to previous experiments.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="max-load-test-1"><strong>Max Load Test</strong><a class="hash-link" href="#max-load-test-1" title="Direct link to heading">​</a></h4><p><strong>Increasing Rate Test:</strong></p><p>When increasing the workflow workers to 128, QPS/burst to 60/70 and shards to 5, the Argo Workflows controller is able to process up to 2100 wor</p><p>kflows/min. Any higher than this seems to run into K8s API Priority and Fairness (APF) limits. </p><p><img loading="lazy" alt="Enter image alt description" src="/assets/images/ko7_Image_14-ef2863d49d1ec7088930b86125bf90e3.png" width="1524" height="700" class="img_ev3q"></p><p>When looking at the K8s API server, we are seeing significant impact with peaks of 1500 writes/sec and 350 reads/sec.</p><p><img loading="lazy" alt="Enter image alt description" src="/assets/images/HH7_Image_15-df838398f527e52ea9f83e49c25f7fa3.png" width="936" height="792" class="img_ev3q"></p><p>When investigating why we are unable to push higher on the K8s API server, we see that APF limits are coming into effect by looking at the apiserver_flowcontrol_current_inqueue_requests. This metric shows the number of requests waiting in the APF flowcontrol queue.</p><p><img loading="lazy" alt="Enter image alt description" src="/assets/images/7RZ_Image_16-d969ed38ee1615d676c3dad927c4d2a0.png" width="773" height="329" class="img_ev3q"></p><p><strong>Queued Reconciliation Test:</strong></p><p>With the max load settings, we observed that the peak workqueue latency is only 20 seconds and the reconcile time is 2 minutes.</p><p><img loading="lazy" alt="Enter image alt description" src="/assets/images/F2t_Image_17-b05c04273726be912e3ba7e4685dd3b2.png" width="1513" height="699" class="img_ev3q"></p><p>The impact on K8s API server is actually less than the previous max load queued reconciliation tests.</p><p><img loading="lazy" alt="Enter image alt description" src="/assets/images/05K_Image_18-2d3c1d9b1066c403190c38cd54b423df.png" width="943" height="795" class="img_ev3q"></p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="observations-from-experiment-4"><strong>Observations from Experiment 4</strong><a class="hash-link" href="#observations-from-experiment-4" title="Direct link to heading">​</a></h4><p>As we did in previous experiments, we again make the comparison between the two different load patterns:</p><table><thead><tr><th>Shards</th><th>Increasing rate test time to reconcile 5000 workflows (mins)</th><th>Reconcile time of 5000 workflows queued all at once (mins)</th></tr></thead><tbody><tr><td>1</td><td>18.5</td><td>18</td></tr><tr><td>2</td><td>9.2</td><td>9</td></tr><tr><td>5</td><td>3.7</td><td>4</td></tr><tr><td>Max load (5 shards)</td><td>2.3</td><td>2</td></tr></tbody></table><p>In general, it appears that submitting all workflows at once performs slightly better than submitting workflows at a steady rate. More experiments will need to be done to further investigate this behavior.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="conclusion"><strong>Conclusion</strong><a class="hash-link" href="#conclusion" title="Direct link to heading">​</a></h2><p>In this blog post we discussed our initial efforts in documenting and understanding the scaling characteristics of the Argo Workflows controller. Our findings show that the existing mechanisms for increasing workflow workers, increasing client and burst QPS settings and sharding the controller can help Argo Workflows scale better. Another interesting observation is that we saw differences in performance with how you submit your workflows.  For the next set of experiments, we plan to evaluate more environmental variables and different types of workflows: multi-step and/or long running. Stay tuned for the report on our next round of experiments and reach out on the CNCF <a href="https://cloud-native.slack.com/archives/C04SURUPDL2" target="_blank" rel="noopener noreferrer">#argo-sig-scalability</a> Slack channel to get help optimizing for your use-cases and scenarios.</p></div><footer class="row docusaurus-mt-lg"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/workflows">workflows</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/benchmarking">benchmarking</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/scalability">scalability</a></li></ul></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><meta itemprop="image" content="https://cnoe.io/assets/images/image30-3953ec55f6ce3f531ad8325ac8a7e0f1.png"><header><h2 class="title_f1Hy" itemprop="headline"><a itemprop="url" href="/blog/argo-cd-application-scalability">Argo CD Benchmarking - Pushing the Limits and Sharding Deep Dive</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2023-11-21T00:00:00.000Z" itemprop="datePublished">November 21, 2023</time> · <!-- -->21 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://github.com/Enclavet" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo" src="https://avatars.githubusercontent.com/u/2309880" alt="Andrew Lee"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://github.com/Enclavet" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Andrew Lee</span></a></div><small class="avatar__subtitle" itemprop="description">Architect, AWS</small></div></div></div><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://github.com/crenshaw-dev" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo" src="https://avatars.githubusercontent.com/u/350466" alt="Michael Crenshaw"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://github.com/crenshaw-dev" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Michael Crenshaw</span></a></div><small class="avatar__subtitle" itemprop="description">Software Engineer, Intuit</small></div></div></div><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://github.com/gauravhub" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo" src="https://avatars.githubusercontent.com/u/60899944" alt="Gaurav Dhamija"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://github.com/gauravhub" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Gaurav Dhamija</span></a></div><small class="avatar__subtitle" itemprop="description">Architect, AWS</small></div></div></div></div></header><div class="markdown" itemprop="articleBody"><h2 class="anchor anchorWithStickyNavbar_LWe7" id="introduction">Introduction<a class="hash-link" href="#introduction" title="Direct link to heading">​</a></h2><p>In <a href="https://aws.amazon.com/blogs/opensource/argo-cd-application-controller-scalability-testing-on-amazon-eks/" target="_blank" rel="noopener noreferrer">Part 1 of our Argo CD benchmarking blog post</a>, we analyzed the impacts of various Argo CD configuration parameters on the performance of Argo CD. In particular we measured the impact of <em>status and operation processes</em>, <em>client QPS, burst QPS</em>, and <em>sharding</em> algorithms on the overall synchronization and reconciliation behavior in Argo CD. We showed that using the right configuration and sharding strategy, particularly by properly setting client and burst QPS, as well as by splitting the workload across multiple workload clusters using Argo CD sharding, overall sync time can be improved by a factor of 4.</p><p>Here, and in Part 2 of our scalability work, we push our scalability experiments for Argo CD further. In particular, among other tests, <em>we run our scalability metrics against a maximum of 500 workload clusters, deploying 50,000 Argo applications</em>. This, to the best of our knowledge, sets the largest scalability testing ever done for Argo CD. We also report on a much deeper set of sharding experiments, utilizing different sharding algorithms for distribution of load across 100 workload clusters. While we report on running our experiments against a legacy sharding algorithm and a round robin algorithm that already exist in <a href="https://github.com/argoproj/argo-cd/releases/tag/v2.8.0" target="_blank" rel="noopener noreferrer">Argo CD 2.8</a>, we also discuss results of workload distribution <em>using 3 new sharding algorithms</em> we developed in collaboration with RedHat, namely: <em>a greedy minimum algorithm</em>, <em>a weighted ring hash algorithm</em>, and <em>a consistent hash with bounded loads algorithm</em>. We show that, depending on the optimization goals one has in mind, choosing from the new sharding algorithms can improve CPU utilization by a factor of 3 and reduce application-to-shard rebalancing by a factor of 5, significantly improving the performance of a highly distributed and massively scaled Argo CD deployment.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="experiment-1-how-client-qpsburst-qps-affects-the-kubernetes-api-server">Experiment 1: How Client QPS/Burst QPS affects the Kubernetes API Server<a class="hash-link" href="#experiment-1-how-client-qpsburst-qps-affects-the-kubernetes-api-server" title="Direct link to heading">​</a></h2><p><strong> <u>Objective:</u> </strong></p><p>The objective of the first experiment is to understand the impact of QPS &amp; Burst Rate parameters on 1/Kubernetes control plane for both the Argo CD cluster and the remote application clusters, and 2/ overall sync duration for Argo CD applications. To understand the impact on Kubernetes API server, we observed following control plane metrics: </p><ul><li>Latency (<code>apiserver_request_duration_seconds_bucket</code>)</li><li>Throughput (<code>apiserver_request_total</code>)</li><li>Error Rate (<code>apiserver_request_total{code=~&quot;[45]..&quot;}</code>) for any request
returning an error code 4xx or 5xx.</li></ul><p>To analyze impact on application synchronization, we observed <code>Sync Duration</code> and <code>No. of Goroutines</code> Argo CD server metrics.</p><p><strong> <u>Test Infrastructure:</u> </strong></p><p>In terms of test infrastructure and workload configuration, we had one central Amazon EKS cluster with Argo CD Server running on it. This central cluster connected with three remote Amazon EKS clusters with each one of them hosting 5000 Argo CD applications. Each application is a Configmap (2KB) provisioned in a dedicated namespace. All of the four clusters, one central and three remote, had a dedicated monitoring stack composed of Prometheus and Grafana installed on them.</p><p><strong> <u>Observations:</u> </strong></p><p><strong> Observation 1 - Impact on Argo CD application synchronization </strong></p><p>The table and graphs below highlight the impact of QPS &amp; Burst Rate on “Sync Duration” as well as the average and maximum no. of goroutines active during the test run.</p><table><tr><td><strong>QPS</strong></td><td><strong>Burst Rate</strong></td><td><strong>Sync Duration</strong></td><td><strong>No. of GoRoutines (Avg)</strong></td><td><strong>No. of GoRoutines (Max)</strong></td></tr><tr><td>50</td><td>100</td><td>61.5 mins</td><td>1760</td><td>1810</td></tr><tr><td>100</td><td>200</td><td>29.5 mins</td><td>2120</td><td>2310</td></tr><tr><td>150</td><td>300</td><td>19.0 mins</td><td>2520</td><td>2760</td></tr><tr><td>200</td><td>400</td><td>18.0 mins</td><td>2620</td><td>2780</td></tr><tr><td>250</td><td>500</td><td>17.5 mins</td><td>2590</td><td>2760</td></tr><tr><td>300</td><td>600</td><td>18.0 mins</td><td>2540</td><td>2760</td></tr></table><p><img loading="lazy" alt="alt_text" src="/assets/images/image19-2c7a2734a0018e389d2c16791da9493f.png" title="image_tooltip" width="1922" height="635" class="img_ev3q"></p><p>To summarize, during the test, we immediately observed ~52% reduction (from 61.5 mins to 29.5 mins) as we increased QPS &amp; Burst Rate from default values to 100 &amp; 200 respectively. This also correlated with corresponding increase in no. of Goroutines processing application synchronization requests. The benefit from increasing values of these parameters started providing diminishing returns with subsequent runs. Beyond QPS &amp; Burst rate of 150 &amp; 300 respectively, there wasn’t measurable improvement observed. This again correlated with number of Goroutines actively processing sync requests.</p><p><strong> Observation 2 - Impact on central Amazon EKS cluster control plane hosting Argo CD Server </strong></p><p>The table and graphs below highlights the impact of QPS &amp; Burst Rate on throughput and latency from Amazon EKS control plane hosting Argo CD Server. We can observe an increase in request rate per second to the Kubernetes control plane which is in line with previous observations related to increase in no. of goroutines processing the sync requests. The increased activity related to sync operations translates into increased requests to Amazon EKS control plane tapering off at QPS of 150 and Burst Rate of 300. Additional increase in QPS and Burst Rate parameters doesn’t noticeably impact request rate per second.</p><table><tr><td><strong>QPS</strong></td><td><strong>Burst Rate</strong></td><td><strong>Request Rate (Max)</strong></td><td><strong>Latency p50 (Max)</strong></td><td><strong>Latency p90 (Max)</strong></td></tr><tr><td>50</td><td>100</td><td>27.2 rps</td><td>13.0 ms</td><td>22.6 ms</td></tr><tr><td>100</td><td>200</td><td>31.9 rps</td><td>13.3 ms</td><td>23.1 ms</td></tr><tr><td>150</td><td>300</td><td>39.8 rps</td><td>14.3 ms</td><td>24.0 ms</td></tr><tr><td>200</td><td>400</td><td>41.4 rps</td><td>14.9 ms</td><td>24.4 ms</td></tr><tr><td>250</td><td>500</td><td>39.0 rps</td><td>15.1 ms</td><td>24.4 ms</td></tr><tr><td>300</td><td>600</td><td>40.7 rps</td><td>16.4 ms</td><td>34.5 ms</td></tr></table><p>From a latency perspective, overall during the course of testing, average (p50) duration remained within range of 13 to 16.5 ms and p90 latency within 22 ms to 34 ms. The error rate remained consistently around ~0.22% with a brief spike to ~0.25% (increase of ~0.03%).</p><p>The relatively low latency numbers and low error rate (<!-- -->&lt;<!-- -->0.25%) indicates that Amazon EKS control plane was able to handle the load comfortably. Increasing QPS and Burst rate only would stretch the control plane to a limited extent indicating it still has resources to process additional requests as long as Argo CD server can generate request traffic.</p><p><img loading="lazy" alt="alt_text" src="/assets/images/image30-3953ec55f6ce3f531ad8325ac8a7e0f1.png" title="image_tooltip" width="1585" height="991" class="img_ev3q"></p><p><strong> Observation 3 - Impact on remote Amazon EKS cluster control plane hosting applications </strong></p><p>We had similar observations regarding latency, throughput and error rate for Amazon EKS control plane of remote application clusters. These are the clusters hosting ~5000 Argo CD applications each and connected to Argo CD Server on the central Amazon EKS cluster. The throughput peaked at ~35 requests per second with QPS and burst rate of 150 &amp; 300 respectively. From an average latency perspective, it remained consistently within single digit millisecond hovering around ~5ms.</p><p><img loading="lazy" alt="alt_text" src="/assets/images/image23-032c5d65156f8d7e03c33abc72423558.png" title="image_tooltip" width="1466" height="987" class="img_ev3q"></p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="experiment-2-revisiting-statusoperation-processors">Experiment 2: Revisiting Status/Operation Processors<a class="hash-link" href="#experiment-2-revisiting-statusoperation-processors" title="Direct link to heading">​</a></h2><p><strong> <u>Objective:</u> </strong></p><p>The objective of the second experiment is to explore why status/operation processors did not have an effect on sync times of our previous experiments. It is possible that the simple nature of ConfigMap applications which takes <!-- -->&lt;<!-- -->1s to deploy is causing this behavior. Most real world applications would consist of tens to hundreds of resources taking longer to be deployed. During this experiment, we will simulate a more complex application which takes longer to deploy than the original ConfigMap application.</p><p><strong> <u>Test Infrastructure:</u> </strong></p><p>Central Argo CD cluster running on a single m5.2xlarge managing 100 application clusters. In order to simulate larger applications, each application will execute a PreSync job which waits 10 seconds before deploying the original ConfigMap application.</p><p>Example of the PreSync Job:</p><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">apiVersion: batch/v1</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">kind: Job</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">metadata:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"> name: before</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"> annotations:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   argocd.argoproj.io/hook: PreSync</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   argocd.argoproj.io/hook-delete-policy: HookSucceeded</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">spec:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"> template:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   spec:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">     containers:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">     - name: sleep</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">       image: alpine:latest</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">       command: [&quot;sleep&quot;, &quot;10&quot;]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">     restartPolicy: Never</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"> backoffLimit: 0</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p><strong> <u>Observations: </u> </strong></p><p><strong> Observation 1 - Syncing never finishes and require a restart of the application controller to continue syncing </strong></p><p>The screenshot below shows that from the start of the sync test at 17:02 till around 17:41, the sync process was deadlocked. We observed no changes to synced apps and the <code>app_operation_processing_queue</code> was pinned at 10k operations.</p><p><img loading="lazy" alt="alt_text" src="/assets/images/image29-df78f9c31065c80389b873ded0acaf77.png" title="image_tooltip" width="1900" height="1184" class="img_ev3q"></p><p>Looking at the Argo CD console for a single application we see that the PreSync job finished 17 mins ago, but the application stayed in the Syncing phase.</p><p><img loading="lazy" alt="alt_text" src="/assets/images/image25-9c1b4406d66014773f0625dd8fa215ee.png" title="image_tooltip" width="1999" height="512" class="img_ev3q"></p><p><strong> Observation 2: There is a link between client QPS/burst QPS and operation/status processor settings </strong></p><p>In order to fix the sync freezing issue, we increased the client QPS/burst QPS from the default 50/100 to 100/200. After the change we were able to collect data on operation/status processor settings.</p><table><tr><td>operation/status processors: <strong>25/50</strong><br>Sync time: <strong>45 mins</strong></td><td>operation/status processors: <strong>50/100</strong><br>Sync time: <strong>30 mins</strong></td></tr><tr><td><img loading="lazy" src="/assets/images/image26-372500eaf0b1d82a4184d7a74939e1a9.png" width="330" alt="alt_text" title="image_tooltip" class="img_ev3q"></td><td><img loading="lazy" src="/assets/images/image16-5a03a19b56ba7917d16c0a757edae247.png" width="330" alt="alt_text" title="image_tooltip" class="img_ev3q"></td></tr></table><p>We can see that there is a link between status/operation processors and client QPS/burst QPS settings. Changing one or the other could be required to improve sync times and Argo CD performance depending on your environment. Our recommendation is to first change the status/operation processor settings. If you run into Argo CD locking up or the performance not increasing further, and you have sufficient resources, you can try increasing the client QPS/burst QPS. But as mentioned in the first experiment, ensure you are monitoring the k8s api-server.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="experiment-3-cluster-scaling">Experiment 3: Cluster Scaling<a class="hash-link" href="#experiment-3-cluster-scaling" title="Direct link to heading">​</a></h2><p><strong> <u>Objective:</u> </strong></p><p>The following experiment is designed to test the compute demands of the Argo CD app controller managing clusters with more than 100 applications.</p><p><strong> <u>Test Infrastructure:</u> </strong></p><p>Central Argo CD cluster with 10 app controller shards running on a single m5.2xlarge node managing 100/250/500 application clusters and 10k 2KB ConfigMap applications.</p><p><strong> <u>Observations:</u> </strong></p><p>From earlier experiences, we can see that when managing 100 clusters, we are close to the limit of a single m5.2xlarge node. As we push further and to 250/500 clusters, we have two observations. The first observation is that the graph data is less smooth than the sync test of 100 clusters. This can indicate that Prometheus is running out of compute as Argo CD is consuming most of it. Please note that we are not using any resource limits/requests in our experiments. If proper resource limits/requests are set, most likely we would only see performance issues with Argo CD and not Prometheus, when operating at the limit of your compute resources. The second observation is that on both the 250/500 cluster tests, there are some drop off in metric data. For the 250 cluster test, there is a blip at the 16:16 mark for Memory Usage. For the 500 cluster test there are blips in data at the 21.05 mark on the Workqueue depth, CPU usage, and Memory usage. In spite of these observations, the sync process completes in a reasonable time.</p><table><tr><td>Clusters: <strong>100</strong><br>Sync time: <strong>9 mins</strong></td><td>Clusters: <strong>250</strong><br>Sync time: <strong>9 mins</strong></td><td>Clusters: <strong>500</strong><br>Sync time: <strong>11 mins</strong></td></tr><tr><td><img loading="lazy" src="/assets/images/image20-0f667cdd92fa25b49f6c3645151250d4.png" width="220" alt="alt_text" title="image_tooltip" class="img_ev3q"></td><td><img loading="lazy" src="/assets/images/image31-7ef7a9bd738d4ff85ac65f5813a2931f.png" width="220" alt="alt_text" title="image_tooltip" class="img_ev3q"></td><td><img loading="lazy" src="/assets/images/image6-4b0c849b965d3b0c16d3f6d42c77e214.png" width="220" alt="alt_text" title="image_tooltip" class="img_ev3q"></td></tr></table>From this experiment, you can see that as you approach the limit of your compute resources, Argo CD and other applications running in your k8s environment could experience issues. It is recommended that you set proper resource limits/requests for your monitoring stack to ensure you have insights into what could be causing your performance issues.<h2 class="anchor anchorWithStickyNavbar_LWe7" id="experiment-4-application-scaling">Experiment 4: Application Scaling<a class="hash-link" href="#experiment-4-application-scaling" title="Direct link to heading">​</a></h2><p><strong> <u>Objective:</u> </strong></p><p>This experiment is meant to push the Argo CD app controller beyond 10k applications. As the previous rounds of experiments were performed with 10k apps, the intention of these experiments is to scale the Argo CD app controller up to 50k apps.</p><p><strong> <u>Test Infrastructure:</u> </strong></p><p>We will be performing this experiment on a Central Argo CD cluster with 10 app controller shards and 500 downstream application clusters. As we scale up the applications up to 10k,15k,20k,25k,30k,50k 2KB ConfigMap applications, we will add additional m5.2xlarge node(s) to the Argo CD cluster. </p><p><strong> <u>Observations:</u> </strong></p><table><tr><td width="33%" valign="top">Sync test at 15k applications with a single m5.2xlarge. You can see blips in data indicating unhealthy behavior on the cluster.</td><td width="33%" valign="top">CPU and Memory Usage is near 100% utilization of 8 vCPUs and 30 GB of memory.</td><td width="33%" valign="top">After adding another node for a total of two m5.2xlarge, we were able to perform a sync in <strong>9 mins</strong>.</td></tr><tr><td valign="top"><img loading="lazy" src="/assets/images/image28-f05b7c6f2cafe65a2ecabc3f549751c6.png" width="220" alt="alt_text" title="image_tooltip" class="img_ev3q"></td><td valign="top"><img loading="lazy" src="/assets/images/image4-c566130260c7093f72cda5f816474e64.png" width="220" alt="alt_text" title="image_tooltip" class="img_ev3q"></td><td valign="top"><img loading="lazy" src="/assets/images/image1-f5f0d2283aa23444889863637678e6e3.png" width="220" alt="alt_text" title="image_tooltip" class="img_ev3q"></td></tr></table><p>After adding another node, we were able to continue our application scaling tests. You can see in the graphs below that syncing 20k and 25k apps was not a problem. The sync test of 30k apps shown on the third graph shows some blips in data, indicating that we are at the limits of two nodes.</p><table><tr><td width="33%" valign="top">Apps: <strong>20000</strong><br>Sync time: <strong>12 mins</strong></td><td width="33%" valign="top">Apps: <strong>25000</strong><br>Sync time: <strong>11 mins</strong></td><td width="33%" valign="top">Apps: <strong>30000</strong><br>Sync time: <strong>19 mins</strong></td></tr><tr><td valign="top"><img loading="lazy" src="/assets/images/image18-e86f74e303eff46a63430cee73365634.png" width="220" alt="alt_text" title="image_tooltip" class="img_ev3q"></td><td valign="top"><img loading="lazy" src="/assets/images/image17-43fedeea99d867624b845ad47f5c76cd.png" width="220" alt="alt_text" title="image_tooltip" class="img_ev3q"></td><td valign="top"><img loading="lazy" src="/assets/images/image21-0c13721b7c5fb5c996b6c161ee83d1d6.png" width="220" alt="alt_text" title="image_tooltip" class="img_ev3q"></td></tr></table><p>For the final test in this experiment, we pushed the cluster to sync 50k apps.</p><table><tr><td width="33%" valign="top">While the cluster was able to manage reconciliation for the 50k apps as shown by a stable Sync Status graph from 8:40, when we start the sync at the 9:02 mark, you can see unhealthy behavior in the graph data.</td><td width="33%" valign="top">From examining the CPU/Memory Usage, you can see we have 100% CPU utilization across the cluster.</td><td width="33%" valign="top">After scaling the cluster to three m5.2xlarge nodes, we were able to perform a sync in <strong>22 mins</strong>.</td></tr><tr><td valign="top"><img loading="lazy" src="/assets/images/image15-eda9682863d46ffb2dd2b165e603005c.png" width="220" alt="alt_text" title="image_tooltip" class="img_ev3q"></td><td valign="top"><img loading="lazy" src="/assets/images/image11-814de637bf2b49628e2ce057fab69829.png" width="220" alt="alt_text" title="image_tooltip" class="img_ev3q"></td><td valign="top"><img loading="lazy" src="/assets/images/image12-16c9df31942c9777bee5f644a081adcc.png" width="220" alt="alt_text" title="image_tooltip" class="img_ev3q"></td></tr></table><p>From the scaling tests, we can see that the Argo CD app controller scales effectively by adding compute resources as we increase the number of applications to sync.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="experiment-5-how-many-shards">Experiment 5: How Many Shards?<a class="hash-link" href="#experiment-5-how-many-shards" title="Direct link to heading">​</a></h2><p><strong> <u>Objective:</u> </strong></p><p>In previous experiments, we utilized ten app controller shards running across multiple nodes. In this experiment, we will explore how the number of app controller shards affect performance.</p><p><strong> <u>Test Infrastructure:</u> </strong></p><p>Central Argo CD cluster with 3, 6, 9 app controller shards running on 3 m5.2xlarge node(s) managing 500 application clusters and 50k 2KB ConfigMap applications.</p><p><strong> <u>Observations:</u> </strong></p><p>For the baseline of three shards it took <strong>75 mins</strong> to perform a sync. Adding additional shards saw further improvements with a sync time of <strong>37 mins</strong> for six shards and a sync time of <strong>21 mins</strong> for nine shards. Further increasing shards beyond nine did not yield any improvements.</p><table><tr><td width="33%" valign="top">Shards: <strong>3</strong><br>Sync time: <strong>75 mins</strong></td><td width="33%" valign="top">Shards: <strong>6</strong><br>Sync time: <strong>37 mins</strong></td><td width="33%" valign="top">Shards: <strong>9</strong><br>Sync time: <strong>21 mins</strong></td></tr><tr><td valign="top"><img loading="lazy" src="/assets/images/image27-1f492fdda9b4cd88e05022ebc284f54c.png" width="220" alt="alt_text" title="image_tooltip" class="img_ev3q"></td><td valign="top"><img loading="lazy" src="/assets/images/image3-210f0699679fc35e094714353e9c27d1.png" width="220" alt="alt_text" title="image_tooltip" class="img_ev3q"></td><td valign="top"><img loading="lazy" src="/assets/images/image8-d97841d73758bb7ca789d40d6989b987.png" width="220" alt="alt_text" title="image_tooltip" class="img_ev3q"></td></tr></table><p>Looking at the CPU and Memory utilization, you can see that adding shards can improve performance only if there are free resources to consume. With the baseline of three shards, CPU utilization of the nodes are well below eight vCPU that each node is allocated. As we add more shards, we can see CPU utilization increasing until we are close to 100% CPU Utilization with nine shards. Adding any more shards would not yield any performance benefits unless we add more nodes.</p><table><tr><td width="33%" valign="top">Shards: <strong>3</strong></td><td width="33%" valign="top">Shards: <strong>6</strong></td><td width="33%" valign="top">Shards: <strong>9</strong></td></tr><tr><td valign="top"><img loading="lazy" src="/assets/images/image7-27ca6026145793a93e0fec41b25e319b.png" width="220" alt="alt_text" title="image_tooltip" class="img_ev3q"></td><td valign="top"><img loading="lazy" src="/assets/images/image13-4e80702d5bec96fa51c265181ea66102.png" width="220" alt="alt_text" title="image_tooltip" class="img_ev3q"></td><td valign="top"><img loading="lazy" src="/assets/images/image9-cd07945b45b6f0fea39c9d38cccaf5ad.png" width="220" alt="alt_text" title="image_tooltip" class="img_ev3q"></td></tr></table><p>From the experiments, the Argo CD app controller sharding mechanism is able to scale as you add more compute resources. Sharding allows both horizontal and vertical scaling. As you add more shards, you can horizontally scale by adding more nodes or vertically scale by utilizing a larger node with more compute resources.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="experiment-6-sharding-deep-dive">Experiment 6: Sharding Deep Dive<a class="hash-link" href="#experiment-6-sharding-deep-dive" title="Direct link to heading">​</a></h2><p><strong> <u>Objective:</u> </strong></p><p>With the release of <a href="https://github.com/argoproj/argo-cd/releases/tag/v2.8.0" target="_blank" rel="noopener noreferrer">Argo CD 2.8</a>, a new sharding algorithm: round-robin was released. The existing legacy sharding algorithm performed a modulo of the number of replicas and the hash sum of the cluster id to determine the shard that should manage the cluster. This led to an imbalance in the number of clusters being managed by each shard. The new round-robin sharding algorithm is supposed to ensure an equal distribution of clusters being managed by each shard. We will also introduce 3 new algorithms: greedy minimum, weighted ring hash, and consistent hash with bounded loads. This experiment will evaluate all the algorithms on shard balance, application distribution and rebalancing on changes to the environment.</p><p><strong> <u>Test Infrastructure:</u> </strong></p><p>Central Argo CD cluster with 10 app controller shards running on 1 m5.2xlarge node managing 100 application clusters and 10k 2KB ConfigMap applications.</p><p><strong> <u>Observations:</u> </strong></p><p>Note: For all the observations, we start monitoring-period when we see items in the operations queue. We end the monitoring-period when all the applications are synced. We then look at the avg metric of CPU/Memory usage during the monitoring-period. </p><p><strong> Legacy </strong> </p><p>The graph below shows the CPU Usage/Memory Usage of the 10 different Argo CD App Controller shards. Looking at the avg, you can see a large variation to how much each shard is utilizing its resources. To make an accurate comparison between the different sharding methods, we calculate <em>the variability</em> by determining the range of the data for both avg CPU usage and Memory usage. The CPU usage variability is calculated by taking the shard with the highest CPU usage and subtracting it from the shard with the least CPU usage: <code>0.55 - 0.23 = 0.32</code>. The Memory usage variability is <code>452 MiB - 225 MiB = 227 MiB</code>.</p><p><strong> Variability: </strong></p><table><tr><td>CPU:</td><td>0.32</td></tr><tr><td>Memory:</td><td>227 MiB</td></tr></table><p><img loading="lazy" alt="alt_text" src="/assets/images/image14-25322b8a6a6ba01d8ae187a22978388b.png" title="image_tooltip" width="1896" height="1132" class="img_ev3q"></p><p><strong> Round-Robin </strong></p><p>With the newly introduced Round-Robin algorithm, you can see improved balance across the shards.</p><p><strong> Variability: </strong></p><table><tr><td>CPU:</td><td>0.02</td></tr><tr><td>Memory:</td><td>110 MiB</td></tr></table><p><img loading="lazy" alt="alt_text" src="/assets/images/image5-3d36436cb1880370b45d3596f9cb4dd7.png" title="image_tooltip" width="1891" height="1137" class="img_ev3q"></p><p><strong> Better but not perfect </strong></p><p>The new round-robin algorithm does a better job of keeping the number of clusters balanced across the shards. But in a real world environment, you would not have an equal number of applications running on each cluster and the work done by each shard is determined not by the number of clusters, but the number of applications. A new experiment was run which deploys a random number of applications to each cluster with the results below. Even with the round-robin algorithm, you can see some high variability in CPU/Memory usage.</p><p><strong> Variability: </strong></p><table><tr><td>CPU:</td><td>0.27</td></tr><tr><td>Memory:</td><td>136 MiB</td></tr></table><p><img loading="lazy" alt="alt_text" src="/assets/images/image24-508c5ce0dc6e0803a16e31fa54b530c0.png" title="image_tooltip" width="1895" height="1134" class="img_ev3q"></p><p><strong> Greedy Minimum Algorithm, sharding by the Number of Apps </strong></p><p>A new algorithm is introduced in order to shard by the number of applications that are running on each cluster. It utilizes a greedy minimum algorithm to always choose the shard with the least number of apps when assigning shards. A description of the algorithm is shown below: </p><p>Iterate through the cluster list:</p><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">1. Determine the number of applications per cluster.</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">2. Find the shard with the least number of applications.</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">3. Add the number of applications to the assigned shard.</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>The same experiment with a random number of applications running on each cluster is run again with the results shown below. With the new algorithm, there is better balance across the shards.</p><p><strong> Variability: </strong></p><table><tr><td>CPU:</td><td>0.06</td></tr><tr><td>Memory:</td><td>109 MiB</td></tr></table><p><img loading="lazy" alt="alt_text" src="/assets/images/image22-7195bed6390399291e7c0c55e647dca6.png" title="image_tooltip" width="1888" height="1128" class="img_ev3q"></p><p>While there is better balance when utilizing the greedy minimum algorithm, there is an issue when changing any aspect of the Argo CD sharding parameters. If you are adding shards, removing shards, adding clusters and/or removing clusters, the algorithm can trigger large scale changes in the shard assignments. Changes to the shard assignments cause shards to waste resources when switching to manage new clusters. This is especially true when utilizing ephemeral clusters in AI/ML training and big data operations where clusters come and go. Starting from the previous experiment from before, we changed the number of shards from 10 to 9 and observed over <strong>75 cluster to shard assignment changes</strong> out of 100 clusters excluding the changes associated with the removed shard. </p><p><strong> Weighted Ring Hash </strong></p><p>In order to decrease the number of shard assignment changes, a well known method called consistent hashing is explored for our use case (<a href="https://www.cs.princeton.edu/courses/archive/fall09/cos518/papers/chash.pdf" target="_blank" rel="noopener noreferrer">Reference</a>). Consistent hashing algorithms utilize a ring hash to determine distribution decisions. This method is already widely utilized by network load balancing applications to evenly distribute traffic in a distributed manner independent of the number of servers/nodes. By utilizing a ring hash algorithm to determine shard assignments, we were able to decrease the number of shard assignment changes when we changed the number of shards from 10 to 9. We observed <strong>48 cluster to shard assignment changes</strong>, excluding the changes associated with the removed shard.</p><p><img loading="lazy" alt="alt_text" src="/assets/images/image10-22b2b37dff8a1b53bed356581ea49fd2.png" title="image_tooltip" width="1906" height="1135" class="img_ev3q"></p><p>To ensure balance, weighting is applied at each shard assignment to ensure the shard with the least number of apps is given the highest weight when choosing shards for assignment. The balancing is not perfect as you can see that CPU variability has increased from the greedy minimum algorithm of 0.06 to 0.12. </p><p><strong> Variability: </strong></p><table><tr><td>CPU:</td><td>0.12</td></tr><tr><td>Memory:</td><td>163 MiB</td></tr></table><p><strong> Consistent Hash with Bounded Loads </strong></p><p>The ring hash algorithm was never designed to allow dynamically updating the weights based on load. While we were able to utilize it for this purpose, we looked at another algorithm called Consistent Hashing with Bounded Loads (<a href="https://arxiv.org/abs/1608.01350" target="_blank" rel="noopener noreferrer">Reference</a>) which looks to solve the problem of consistent hashing and load uniformity. By utilizing this new algorithm, we were able to significantly decrease the redistribution of cluster to shard assignments. When we change the number of shards from 10 to 9, we only observed <strong>15 cluster to shard assignment changes</strong> excluding the changes associated with the removed shard.</p><p><img loading="lazy" alt="alt_text" src="/assets/images/image2-24e93b36ede5b0e7ed3b6322f1717013.png" title="image_tooltip" width="1907" height="1135" class="img_ev3q"></p><p>The trade off is slightly worse cluster/app balancing than the weighted ring hash which increased CPU variability from 0.12 to 0.17.</p><p><strong> Variability: </strong></p><table><tr><td>CPU:</td><td>0.17</td></tr><tr><td>Memory:</td><td>131 MiB</td></tr></table><p>There are no direct recommendations about which algorithm you should utilize, as each of them have their pros and cons. You should evaluate each for your environment whether you are looking for strict balancing of clusters/apps across the shards or whether you want to minimize the impact of making frequent changes to your Argo CD environment. </p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="conclusion">Conclusion<a class="hash-link" href="#conclusion" title="Direct link to heading">​</a></h2><p>In this blog post, we continued our scalability tests of the Argo CD app controller by answering some questions we had from our first scalability tests about the common scalability parameters. We showed how QPS/Burst QPS affects the k8s api server, determined why status/operation processors did not affect our previous scalability tests, and how those parameters are linked together. We then continued our scalability tests by pushing the Argo CD app controller to 500 clusters and 50,000 apps. We ended our tests by showing that a key component of scaling the Argo CD app controller is how it performs sharding. By doing a deep dive into how the app controller performs sharding we also determined some ways to improve sharding by adding in and evaluating new sharding algorithms. We are currently evaluating how to contribute these changes back to Argo CD. Stay tuned for those contributions and reach out on the CNCF<a href="https://cloud-native.slack.com/archives/C04SURUPDL2" target="_blank" rel="noopener noreferrer"> #argo-sig-scalability</a> or the <a href="https://cloud-native.slack.com/archives/C05TN9WFN5S" target="_blank" rel="noopener noreferrer">#cnoe-interest</a> Slack channel to get help optimizing for your use-cases and scenarios.</p></div><footer class="row docusaurus-mt-lg"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/argocd">argocd</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/benchmarking">benchmarking</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/scalability">scalability</a></li></ul></div></footer></article><nav class="pagination-nav" aria-label="Blog list page navigation"></nav></main></div></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">DOCS</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/docs/intro">Introduction</a></li></ul></div><div class="col footer__col"><div class="footer__title">SOCIAL</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://cloud-native.slack.com/archives/C05TN9WFN5S" target="_blank" rel="noopener noreferrer" class="footer__link-item">Slack<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://calendar.google.com/calendar/u/0/embed?src=064a2adfce866ccb02e61663a09f99147f22f06374e7a8994066bdc81e066986@group.calendar.google.com&amp;ctz=America/Los_Angeles" target="_blank" rel="noopener noreferrer" class="footer__link-item">Community Meeting Calendar<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div><div class="col footer__col"><div class="footer__title">MORE</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/blog">Blog</a></li><li class="footer__item"><a href="https://github.com/cnoe-io" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2024 CNOE</div></div></div></footer></div>
<script src="/assets/js/runtime~main.b0d8bc33.js"></script>
<script src="/assets/js/main.e84582e8.js"></script>
</body>
</html>